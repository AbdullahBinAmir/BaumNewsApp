{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "import wikipediaapi\n",
    "import feedparser\n",
    "import signal\n",
    "from dateutil import parser\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading previous data\n",
    "if os.path.exists(\"data/Dataset.csv\"):\n",
    "    org_data = pd.read_csv(\"data/Dataset.csv\")\n",
    "    org_data.drop(columns=['id'], axis=1, inplace=True)\n",
    "else:\n",
    "    org_data = pd.DataFrame()\n",
    "\n",
    "# reading links file\n",
    "links = pd.read_excel(\"data/Sources.xlsx\", header=1)\n",
    "wiki_links = pd.read_excel('data/Sources.xlsx', sheet_name='Wiki Categories', header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting new data from newsapi and processing it \n",
    "\n",
    "# news api url\n",
    "URL = \"https://newsapi.org/v2/everything?\"\n",
    "\n",
    "links['News API 2'] = links['News API 2'].apply(lambda x: str(x).split('=')[1] if x is not np.nan else np.nan)\n",
    "\n",
    "#creating a dataframe\n",
    "dataset_newsapi = pd.DataFrame()\n",
    "\n",
    "# getting data from each link\n",
    "for index, link_item in enumerate(links['News API Link'].values):\n",
    "    # parameters for request\n",
    "    PARAMS = {\n",
    "    'domains': f\"{link_item}\",\n",
    "    'apikey' : \"31a644adf7964db285900d5fc0cd2f30\" if str(links['News API 2'].iloc[index]) is not None else \"31a644adf7964db285900d5fc0cd2f30\"\n",
    "    }\n",
    "\n",
    "    # sending get request and saving the response as response object\n",
    "    r = requests.get(url = URL, params = PARAMS)\n",
    "    \n",
    "    # extracting data in json format\n",
    "    data = r.json()\n",
    "    if 'totalResults' in data.keys():    \n",
    "        if data[\"totalResults\"] > 1:\n",
    "            json_data = pd.read_json(json.dumps(data))\n",
    "            dict_df = pd.json_normalize(json_data['articles'])\n",
    "            dataset_newsapi = pd.concat([dataset_newsapi, dict_df], ignore_index=True)\n",
    "            dataset_newsapi['Paywall Value'] = links['Paywall Value'].iloc[index] if links['Paywall Value'].iloc[index] is not np.nan else np.nan\n",
    "    else:\n",
    "        continue\n",
    "    if dataset_newsapi.shape[0] > 0:\n",
    "        break\n",
    "dataset_newsapi['publishedAt'] = dataset_newsapi['publishedAt'].apply(lambda x: str(parser.parse(x, fuzzy=True).strftime(\"%d-%m-%Y\")) if x is not np.nan else np.nan)\n",
    "dataset_newsapi.rename(columns={\"source.name\": \"source_name\"}, inplace=True)\n",
    "dataset_newsapi = dataset_newsapi[['source_name', 'title', 'description', 'url', 'publishedAt', 'author', 'Paywall Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57346/3903582661.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fetch_rss_df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# getting new data from RSS Feed\n",
    "\n",
    "fetch_rss_df = links[[\"Website Name\", \"RSS Feed Link\", \"Paywall Value\"]]\n",
    "fetch_rss_df.dropna(inplace=True)\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise Exception(\"Execution timed out\")\n",
    "\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "\n",
    "# set a maximum time limit of 10 seconds for each line of code\n",
    "signal.alarm(120)\n",
    "\n",
    "\n",
    "dataset_feedrss = pd.DataFrame()\n",
    "for i in range(fetch_rss_df.shape[0]):\n",
    "    signal.alarm(10)\n",
    "    rss_url = fetch_rss_df['RSS Feed Link'].iloc[i] # replace with the URL of your RSS feed\n",
    "    try:\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        articles = []\n",
    "        for entry in feed.entries:\n",
    "            article = {\n",
    "            \"source_name\" : fetch_rss_df['Website Name'].iloc[i],\n",
    "            \"title\": entry.title if \"title\" in entry.keys() else None,\n",
    "            \"url\": entry.link if \"link\" in entry.keys() else None,\n",
    "            \"publishedAt\": entry.published if \"published\" in entry.keys() else (entry.pubDate if \"pubDate\" in entry.keys() else None),\n",
    "            \"author\" : entry.author if \"author\" in entry.keys() else None,\n",
    "            \"description\" : entry.description if \"description\" in entry.keys() else None\n",
    "            }\n",
    "            articles.append(article)\n",
    "        dataset_feedrss = pd.concat([dataset_feedrss, pd.DataFrame(articles)])\n",
    "        dataset_feedrss['Paywall Value'] = fetch_rss_df['Paywall Value'].iloc[i] if fetch_rss_df['Paywall Value'].iloc[i] is not np.nan else np.nan\n",
    "        signal.alarm(0)\n",
    "    except:\n",
    "        signal.alarm(0)\n",
    "    break\n",
    "dataset_feedrss['publishedAt'] = dataset_feedrss['publishedAt'].apply(lambda x: str(parser.parse(x, fuzzy=True).strftime(\"%d-%m-%Y\")) if x is not np.nan else np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of data from wikipedia\n",
    "\n",
    "\n",
    "# Set up the Wikipedia API\n",
    "wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "dataset_wiki = pd.DataFrame()\n",
    "for i, category in enumerate(wiki_links['Wiki Categories'].values):\n",
    "    try:\n",
    "        # Set the category you want to download articles from\n",
    "        category_name = 'Category:{}'.format(category)\n",
    "\n",
    "        # Get the category page and its members (articles)\n",
    "        category_page = wiki.page(category_name)\n",
    "        category_members = category_page.categorymembers\n",
    "\n",
    "        # Loop through the members and download each article\n",
    "        articles = []\n",
    "        for member in category_members.values():\n",
    "            # Check if the member is an article (not a subcategory)\n",
    "            if member.ns == wikipediaapi.Namespace.MAIN:\n",
    "                # Get the article content and save it to a file\n",
    "                article = {\n",
    "                    \"source_name\" : 'wikipedia',\n",
    "                    \"title\" : member.title,\n",
    "                    'description' : wiki.page(member.title).text.split('\\n')[0],\n",
    "                    'url' : wiki.page(member.title).fullurl,\n",
    "                    'publishedAt' : None,\n",
    "                    'author' : None,\n",
    "                    'Paywall Value' : None\n",
    "                }\n",
    "                articles.append(article)\n",
    "        dataset_wiki = pd.concat([dataset_wiki, pd.DataFrame(articles)])\n",
    "        # print(\"index {}\".format(str(dataset_wiki.shape[0])+ \" : \" + str(i)), end=\"\\r\")\n",
    "    except:\n",
    "        pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset_newsapi, dataset_feedrss, dataset_wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 232 entries, 0 to 231\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             232 non-null    int64  \n",
      " 1   source_name    232 non-null    object \n",
      " 2   title          232 non-null    object \n",
      " 3   description    216 non-null    object \n",
      " 4   url            232 non-null    object \n",
      " 5   publishedAt    96 non-null     object \n",
      " 6   author         80 non-null     object \n",
      " 7   Paywall Value  16 non-null     float64\n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 16.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, org_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_index(drop=False, inplace=True)\n",
    "dataset.rename(columns={\"index\": \"id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop_duplicates()\n",
    "dataset.to_csv(\"data/Dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=[\"description\", \"url\", \"Paywall Value\", \"source_name\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['tags'] = dataset['author'].fillna('')+ \" \" + dataset['title'].fillna('')+ \" \" + dataset['publishedAt'].fillna('')\n",
    "dataset.drop(columns=[\"author\", \"title\", \"publishedAt\"], inplace=True)\n",
    "dataset[\"tags\"] = dataset[\"tags\"].apply(lambda x: x.lower())\n",
    "dataset[\"tags\"] = dataset[\"tags\"].apply(lambda x: x.replace(\"— \", \"\"))\n",
    "dataset[\"tags\"] = dataset[\"tags\"].apply(lambda x: x.replace(\":\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"data/Tags.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer  = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(dataset['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/newsapi_articles_cosinesimilarity.pkl']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# save cosine similarity in a model file\n",
    "joblib.dump(similarity, 'model/newsapi_articles_cosinesimilarity.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys_articles_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8cf9a2f6ed740e1f7cb7ddc50fbb67ffc26c340ece827f9dc94a229b429bc69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
